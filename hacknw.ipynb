{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "# from urlparse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cli_green(fn):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return \"\\033[92m\"+fn()\n",
    "    return wrapped\n",
    "\n",
    "def scrape_topic_page(topic_number):\n",
    "    if not isinstance(topic_number, (basestring, int)):\n",
    "        raise ValueError(\"Topic number has to be int or basestring\")\n",
    "    soup = BeautifulSoup(\n",
    "        requests.get(\n",
    "            'http://netwars.pl/temat/{0!s}'.format(topic_number)\n",
    "        ).text\n",
    "    )\n",
    "    return soup \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "class NwTopic(object):\n",
    "    \n",
    "    NW_URL = 'http://netwars.pl/temat/'\n",
    "    \n",
    "    def __init__(self, topic_raw_html, topic_number):\n",
    "        \n",
    "        self.soup = BeautifulSoup(topic_raw_html)\n",
    "        self.topic_number = topic_number\n",
    "        \n",
    "        if 'Nie znaleziono tematu' in soup:\n",
    "            raise ValueError('topic does not exist')\n",
    "                     \n",
    "                    \n",
    "    @classmethod\n",
    "    def from_url(cls, topic_number):\n",
    "        r = requests.get(cls.NW_URL+str(topic_number))    \n",
    "        if r.status_code != 200:\n",
    "            raise ValueError('failed on get-request')\n",
    "        return cls(r.text, topic_number)\n",
    "\n",
    "    @classmethod\n",
    "    def from_html(cls, topic_raw_html, topic_number):\n",
    "        \"\"\"\n",
    "        To use with already scraped data, for perfromance reasons\n",
    "        \"\"\"\n",
    "        return cls(topic_raw_html, topic_number)\n",
    "    \n",
    "    @property\n",
    "    def posts(self):\n",
    "        ix = \"topic_{0!s}\".format(self.topic_number)\n",
    "        post_list = soup.findAll(\"div\",{\"id\":ix})\n",
    "        return post_list\n",
    "#         if len(post_list) != 1:\n",
    "#             raise ValueError('Cos tu jest nie tak')\n",
    "#         return post_list\n",
    "\n",
    "    \n",
    "    \n",
    "class NW(object):\n",
    "    BASE_URL = 'http://netwars.pl/'\n",
    "    OT_FORUM_NUMBER = '4'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s(%r)'%(self.__class__, self.username)\n",
    "     \n",
    "    def __init__(self, username, password):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.logged_in = False\n",
    "\n",
    "    def login(self):\n",
    "        payload = {\n",
    "            'tnick':self.username,\n",
    "            'tpass':self.password\n",
    "        }\n",
    "        \n",
    "        nwsession = requests.session()\n",
    "        nwsession.post(urljoin(self.BASE_URL,'login'), payload)\n",
    "        self.nwsession = nwsession\n",
    "        self.logged_in = True\n",
    "    \n",
    "    def logout(self):\n",
    "        self.nwsession.post(urljoin(self.BASE_URL,'logout'))\n",
    "        self.logged_in = False\n",
    "        \n",
    "    def topics(self):\n",
    "        forum = self.nwsession.get(\n",
    "            self.BASE_URL+'forum/'+self.OT_FORUM_NUMBER).text\n",
    "        soup = BeautifulSoup(forum)\n",
    "        return [topic.text for topic in soup.findAll('td',{'class':'topic'})]\n",
    "\n",
    "\n",
    "def BeatNW(self):\n",
    "    FORUM_URL = 'http//netwars.pl'\n",
    "    \n",
    "\n",
    "# r = session.get('http://netwars.pl')\n",
    "# nw_time = parser.parse(r.headers['date']).replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "topic = NwTopic.from_url(173454)\n",
    "t = topic.soup.find_all('div', {'id':'topic_173454'})[0]\n",
    "all_posts = t.findAll('div',class_='post')\n",
    "collect_all_post = []\n",
    "for post in all_posts:\n",
    "    POST = {}\n",
    "    POST['post_number'] = int(post['id'].split('_')[-1])\n",
    "    POST['topic_number'] = 173454\n",
    "    for subpost in post:\n",
    "        if 'posthead' in subpost['class']:\n",
    "            for sub_head in subpost.children:\n",
    "                if 'p2_data' in sub_head['class']:\n",
    "                    POST['date'] = sub_head.text\n",
    "                if 'p2_nick' in sub_head['class']:\n",
    "                    POST['user_href'] = sub_head.a['href']\n",
    "                    POST['user_name'] = sub_head.a.text\n",
    "                \n",
    "        if 'post_body' in subpost['class']:\n",
    "            POST['post_body'] = subpost.text\n",
    "    collect_all_post.append(POST)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ids = topic.soup.findAll('div',{'class':'post'})\n",
    "dates = topic.soup.findAll('div',{'class':'p2_data'})\n",
    "nicks = topic.soup.findAll('div',{'class':'p2_nick'})\n",
    "bodies = topic.soup.findAll('div',{'class':'post_body'})\n",
    "\n",
    "dates = map(lambda x: x.text, dates)\n",
    "post_bodies = map(lambda x: x.text, bodies)\n",
    "user_hrefs = map(lambda x: x.a['href'], nicks)\n",
    "user_names = map(lambda x: x.a.text, nicks)\n",
    "ids = map(lambda x: x['id'].split('_')[-1], ids)\n",
    "\n",
    "super_json_kurwo = [\n",
    "    {\n",
    "    'post_id':pid, \n",
    "    'post_date':pdate, \n",
    "    'user_href':href, \n",
    "    'user_name':uname, \n",
    "    'post_body':body\n",
    "    } for pid,pdate,href,uname,body in \n",
    "     zip(ids, dates, user_hrefs, user_names, post_bodies)\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://netwars.pl')\n",
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'Dzisiaj, 15:43:26',\n",
       " 'post_body': 'W 76 SPQR.Colos napisał: [Pokaż]\\n\\n\\r\\nCoś sie obawiam ze to nie twoja sprawa colos.',\n",
       " 'post_number': 100,\n",
       " 'topic_number': 173454,\n",
       " 'user_href': '/user/28979',\n",
       " 'user_name': 'ajnos'}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytz import utc\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "from apscheduler.jobstores.redis import RedisJobStore\n",
    "from apscheduler.executors.pool import ProcessPoolExecutor\n",
    "\n",
    "jobstores = {\n",
    "#     'mongo': {'type': 'mongodb'},\n",
    "#     'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "    'default': RedisJobStore()\n",
    "}\n",
    "executors = {\n",
    "    'default': {'type': 'threadpool', 'max_workers': 20},\n",
    "    'processpool': ProcessPoolExecutor(max_workers=5)\n",
    "}\n",
    "\n",
    "job_defaults = {\n",
    "    'coalesce': False,\n",
    "    'max_instances': 3\n",
    "}\n",
    "# scheduler = BlockingScheduler()\n",
    "scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "import hashlib\n",
    "import requests\n",
    "L = []\n",
    "def get_nw_hash():\n",
    "    hash_string = hashlib.md5(requests.get('http://netwars.pl').text.encode('utf8')).hexdigest()\n",
    "    \n",
    "    if hash_string in L:\n",
    "        L.append('oh now nw changed!')\n",
    "    else:\n",
    "        L.append(hash_string)    \n",
    "\n",
    "scheduler.add_job(get_nw_hash, 'interval', seconds=30)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw_job = scheduler.get_jobs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('http://localhost:8000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = redis.Redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.lpush('ok909-0',['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'ok909-0', b'ok', b'rq:queues', b'rq:finished:default']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.set('kdjkdjkdjd','3202032020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'3202032020'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get('kdjkdjkdjd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scheduler.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scheduler.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
