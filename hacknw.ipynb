{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_post_bodies(bodies):\n",
    "    for body in bodies:\n",
    "        cites = []\n",
    "        cited = body.findAll('div', {'class': 'cite'})\n",
    "        if cited:\n",
    "            cites = []\n",
    "            for c in cited:\n",
    "                cites.append(c['name'])\n",
    "        collect_text = []\n",
    "        for tag in body:\n",
    "            if tag.name not in ('div', 'p'):\n",
    "                if hasattr(tag, 'text'):\n",
    "                    collect_text.append(tag.text)\n",
    "                if isinstance(tag, NavigableString):\n",
    "                    collect_text.append(str(tag))\n",
    "\n",
    "        else:\n",
    "            yield ''.join(collect_text), cites\n",
    "\n",
    "\n",
    "class NW(object):\n",
    "    BASE_URL = 'http://netwars.pl/'\n",
    "    BASE_URL_TOPIC = 'http://netwars.pl/temat/{!s}'\n",
    "    OT_FORUM_NUMBER = '4'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__, self.username)\n",
    "\n",
    "    def __init__(self, username=None, password=None):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.logged_in = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _url_to_soup(url):\n",
    "        return BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "\n",
    "    @staticmethod\n",
    "    def _topic_differences(old, new):\n",
    "        \"\"\"\n",
    "        Return topics that changed between two scrapes\n",
    "        \"\"\"\n",
    "        return dict((set(new.items()) - set(old.items()))).keys()\n",
    "\n",
    "    @staticmethod\n",
    "    def _live_user_differences(old, new):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param old: old version of nw-meta to be compared against \n",
    "        :param new: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return set(old) == set(new)\n",
    "\n",
    "    @staticmethod\n",
    "    def _topic_soup_to_json(soup):\n",
    "\n",
    "        if 'Nie znaleziono' in soup.text:\n",
    "            raise ValueError('Topic does not exist')\n",
    "\n",
    "        topic_number = list(filter(\n",
    "            lambda x: 'topic_' in x, [\n",
    "                d.get('id', 'not-relevant')\n",
    "                for d in soup.findAll('div')\n",
    "                ]))\n",
    "\n",
    "        topic_id = int(topic_number[0].split('_')[-1])\n",
    "        navi_list = [a for a in soup.findAll('ul', {'class': 'forum_navi'})][0].findAll('li')\n",
    "        forum_id = navi_list[1].a['href']\n",
    "        topic_name = navi_list[2].text\n",
    "\n",
    "        ids = soup.findAll('div', {'class': 'post'})\n",
    "        dates = soup.findAll('div', {'class': 'p2_data'})\n",
    "        nicks = soup.findAll('div', {'class': 'p2_nick'})\n",
    "        bodies = soup.findAll('div', {'class': 'post_body'})\n",
    "\n",
    "        dates = map(lambda x: x.text, dates)\n",
    "        post_bodies = process_post_bodies(bodies)\n",
    "        user_hrefs = map(lambda x: x.a['href'], nicks)\n",
    "        user_names = map(lambda x: x.a.text, nicks)\n",
    "        ids = map(lambda x: x['id'].split('_')[-1], ids)\n",
    "\n",
    "        posts_list = [\n",
    "            {\n",
    "                'topic_id': topic_id,\n",
    "                'forum_id': forum_id,\n",
    "                'post_id': pid,\n",
    "                'post_date': pdate,\n",
    "                'user_href': href,\n",
    "                'user_name': uname,\n",
    "                'post_body': body,\n",
    "                'cites': cites\n",
    "\n",
    "            } for pid, pdate, href, uname, (body, cites) in\n",
    "            zip(ids, dates, user_hrefs, user_names, post_bodies)\n",
    "            ]\n",
    "\n",
    "        topic_meta = {\n",
    "            'forum_id': forum_id,\n",
    "            'topic_name': topic_name,\n",
    "            'topic_id': topic_id\n",
    "\n",
    "        }\n",
    "\n",
    "        return posts_list, topic_meta\n",
    "\n",
    "    def login(self):\n",
    "        if not self.username or notself.password:\n",
    "            raise ValueError('No cred')\n",
    "\n",
    "        payload = {\n",
    "            'tnick': self.username,\n",
    "            'tpass': self.password\n",
    "        }\n",
    "\n",
    "        nwsession = requests.session()\n",
    "        nwsession.post(urljoin(self.BASE_URL, 'login'), payload)\n",
    "        self.nwsession = nwsession\n",
    "        self.logged_in = True\n",
    "\n",
    "    def logout(self):\n",
    "        self.nwsession.post(urljoin(self.BASE_URL, 'logout'))\n",
    "        self.logged_in = False\n",
    "\n",
    "    def topic_to_json(self, topic_number):\n",
    "        soup = self._url_to_soup(self.BASE_URL_TOPIC.format(topic_number))\n",
    "        return self._topic_soup_to_json(soup)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list_of_active_users(base_soup):\n",
    "        return [a['href'] for a in base_soup.findAll(\n",
    "            'div', attrs={'id': 'footer'}\n",
    "        )[0].findAll('a')][2:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _topics_and_post_number(base_soup):\n",
    "        topic_ids = [int(z.a['href'].split('/')[-1]) for z in base_soup.findAll('td', {'class': 'topic'})]\n",
    "        number_of_posts = [int(z.text) for z in s.findAll('td', {'class': 'posts'})]\n",
    "        return dict(zip(topic_ids, number_of_posts))\n",
    "\n",
    "    def home_page_status(self):\n",
    "        soup = self._url_to_soup(self.BASE_URL)\n",
    "        topics = self._topics_and_post_number(soup)\n",
    "        users = self._list_of_active_users(soup)\n",
    "        return topics, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pytz import utc\n",
    "# from apscheduler.schedulers.background import BackgroundScheduler\n",
    "# from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "# from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "# from apscheduler.jobstores.redis import RedisJobStore\n",
    "# from apscheduler.executors.pool import ProcessPoolExecutor\n",
    "\n",
    "# jobstores = {\n",
    "# #     'mongo': {'type': 'mongodb'},\n",
    "# #     'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "#     'default': RedisJobStore()\n",
    "# }\n",
    "# executors = {\n",
    "#     'default': {'type': 'threadpool', 'max_workers': 20},\n",
    "#     'processpool': ProcessPoolExecutor(max_workers=5)\n",
    "# }\n",
    "\n",
    "# job_defaults = {\n",
    "#     'coalesce': False,\n",
    "#     'max_instances': 3\n",
    "# }\n",
    "# # scheduler = BlockingScheduler()\n",
    "# scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "# import hashlib\n",
    "# import requests\n",
    "# L = []\n",
    "# def get_nw_hash():\n",
    "#     hash_string = hashlib.md5(requests.get('http://netwars.pl').text.encode('utf8')).hexdigest()\n",
    "    \n",
    "#     if hash_string in L:\n",
    "#         L.append('oh now nw changed!')\n",
    "#     else:\n",
    "#         L.append(hash_string)    \n",
    "\n",
    "# scheduler.add_job(get_nw_hash, 'interval', seconds=30)\n",
    "# scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw = NW()\n",
    "posts, head = nw.topic_to_json(173399)\n",
    "# \n",
    "# local_machine = {'host':'localhost','port':9200}\n",
    "# es = elasticsearch.Elasticsearch(hosts=[local_machine])\n",
    "# \n",
    "# for i,p in enumerate(posts):  \n",
    "#     es.index(index='nw', doc_type='post', id=i, body=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from count_w import count_words_at_url\n",
    "from bla import bla\n",
    "from redis import Redis\n",
    "from rq import Queue, job\n",
    "q = Queue(connection=Redis(),name='blabla2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = q.enqueue(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50fb7127-4e0d-48cf-ac83-5ddd5f62bf09'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50fb7127-4e0d-48cf-ac83-5ddd5f62bf09'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = job.Job.fetch('50fb7127-4e0d-48cf-ac83-5ddd5f62bf09', connection=Redis())\n",
    "j.is_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oooooooooook'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.result"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}