{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_post_bodies(bodies):\n",
    "    for body in bodies:\n",
    "        cites = []\n",
    "        cited = body.findAll('div', {'class': 'cite'})\n",
    "        if cited:\n",
    "            cites = []\n",
    "            for c in cited:\n",
    "                cites.append(c['name'])\n",
    "        collect_text = []\n",
    "        for tag in body:\n",
    "            if tag.name not in ('div', 'p'):\n",
    "                if hasattr(tag, 'text'):\n",
    "                    collect_text.append(tag.text)\n",
    "                if isinstance(tag, NavigableString):\n",
    "                    collect_text.append(str(tag))\n",
    "\n",
    "        else:\n",
    "            yield ''.join(collect_text), cites\n",
    "\n",
    "\n",
    "class NW(object):\n",
    "    BASE_URL = 'http://netwars.pl/'\n",
    "    BASE_URL_TOPIC = 'http://netwars.pl/temat/{!s}'\n",
    "    OT_FORUM_NUMBER = '4'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__, self.username)\n",
    "\n",
    "    def __init__(self, username=None, password=None):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.logged_in = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _url_to_soup(url):\n",
    "        return BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "\n",
    "    @staticmethod\n",
    "    def _topic_differences(old, new):\n",
    "        \"\"\"\n",
    "        Return topics that changed between two scrapes\n",
    "        \"\"\"\n",
    "        return dict((set(new.items()) - set(old.items()))).keys()\n",
    "\n",
    "    @staticmethod\n",
    "    def _live_user_differences(old, new):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param old: old version of nw-meta to be compared against \n",
    "        :param new: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return set(old) == set(new)\n",
    "\n",
    "    @staticmethod\n",
    "    def _topic_soup_to_json(soup):\n",
    "\n",
    "        if 'Nie znaleziono' in soup.text:\n",
    "            raise ValueError('Topic does not exist')\n",
    "\n",
    "        topic_number = list(filter(\n",
    "            lambda x: 'topic_' in x, [\n",
    "                d.get('id', 'not-relevant')\n",
    "                for d in soup.findAll('div')\n",
    "                ]))\n",
    "\n",
    "        topic_id = int(topic_number[0].split('_')[-1])\n",
    "        navi_list = [a for a in soup.findAll('ul', {'class': 'forum_navi'})][0].findAll('li')\n",
    "        forum_id = navi_list[1].a['href']\n",
    "        topic_name = navi_list[2].text\n",
    "\n",
    "        ids = soup.findAll('div', {'class': 'post'})\n",
    "        dates = soup.findAll('div', {'class': 'p2_data'})\n",
    "        nicks = soup.findAll('div', {'class': 'p2_nick'})\n",
    "        bodies = soup.findAll('div', {'class': 'post_body'})\n",
    "\n",
    "        dates = map(lambda x: x.text, dates)\n",
    "        post_bodies = process_post_bodies(bodies)\n",
    "        user_hrefs = map(lambda x: x.a['href'], nicks)\n",
    "        user_names = map(lambda x: x.a.text, nicks)\n",
    "        ids = map(lambda x: x['id'].split('_')[-1], ids)\n",
    "\n",
    "        posts_list = [\n",
    "            {\n",
    "                'topic_id': topic_id,\n",
    "                'forum_id': forum_id,\n",
    "                'post_id': pid,\n",
    "                'post_date': pdate,\n",
    "                'user_href': href,\n",
    "                'user_name': uname,\n",
    "                'post_body': body,\n",
    "                'cites': cites\n",
    "\n",
    "            } for pid, pdate, href, uname, (body, cites) in\n",
    "            zip(ids, dates, user_hrefs, user_names, post_bodies)\n",
    "            ]\n",
    "\n",
    "        topic_meta = {\n",
    "            'forum_id': forum_id,\n",
    "            'topic_name': topic_name,\n",
    "            'topic_id': topic_id\n",
    "\n",
    "        }\n",
    "\n",
    "        return posts_list, topic_meta\n",
    "\n",
    "    def login(self):\n",
    "        if not self.username or notself.password:\n",
    "            raise ValueError('No cred')\n",
    "\n",
    "        payload = {\n",
    "            'tnick': self.username,\n",
    "            'tpass': self.password\n",
    "        }\n",
    "\n",
    "        nwsession = requests.session()\n",
    "        nwsession.post(urljoin(self.BASE_URL, 'login'), payload)\n",
    "        self.nwsession = nwsession\n",
    "        self.logged_in = True\n",
    "\n",
    "    def logout(self):\n",
    "        self.nwsession.post(urljoin(self.BASE_URL, 'logout'))\n",
    "        self.logged_in = False\n",
    "\n",
    "    def topic_to_json(self, topic_number):\n",
    "        soup = self._url_to_soup(self.BASE_URL_TOPIC.format(topic_number))\n",
    "        return self._topic_soup_to_json(soup)\n",
    "\n",
    "    @staticmethod\n",
    "    def _list_of_active_users(base_soup):\n",
    "        return [a['href'] for a in base_soup.findAll(\n",
    "            'div', attrs={'id': 'footer'}\n",
    "        )[0].findAll('a')][2:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _topics_and_post_number(base_soup):\n",
    "        topic_ids = [int(z.a['href'].split('/')[-1]) for z in base_soup.findAll('td', {'class': 'topic'})]\n",
    "        number_of_posts = [int(z.text) for z in s.findAll('td', {'class': 'posts'})]\n",
    "        return dict(zip(topic_ids, number_of_posts))\n",
    "\n",
    "    def home_page_status(self):\n",
    "        soup = self._url_to_soup(self.BASE_URL)\n",
    "        topics = self._topics_and_post_number(soup)\n",
    "        users = self._list_of_active_users(soup)\n",
    "        return topics, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pytz import utc\n",
    "# from apscheduler.schedulers.background import BackgroundScheduler\n",
    "# from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "# from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "# from apscheduler.jobstores.redis import RedisJobStore\n",
    "# from apscheduler.executors.pool import ProcessPoolExecutor\n",
    "\n",
    "# jobstores = {\n",
    "# #     'mongo': {'type': 'mongodb'},\n",
    "# #     'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "#     'default': RedisJobStore()\n",
    "# }\n",
    "# executors = {\n",
    "#     'default': {'type': 'threadpool', 'max_workers': 20},\n",
    "#     'processpool': ProcessPoolExecutor(max_workers=5)\n",
    "# }\n",
    "\n",
    "# job_defaults = {\n",
    "#     'coalesce': False,\n",
    "#     'max_instances': 3\n",
    "# }\n",
    "# # scheduler = BlockingScheduler()\n",
    "# scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "# import hashlib\n",
    "# import requests\n",
    "# L = []\n",
    "# def get_nw_hash():\n",
    "#     hash_string = hashlib.md5(requests.get('http://netwars.pl').text.encode('utf8')).hexdigest()\n",
    "    \n",
    "#     if hash_string in L:\n",
    "#         L.append('oh now nw changed!')\n",
    "#     else:\n",
    "#         L.append(hash_string)    \n",
    "\n",
    "# scheduler.add_job(get_nw_hash, 'interval', seconds=30)\n",
    "# scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nw = NW()\n",
    "posts, head = nw.topic_to_json(173399)\n",
    "# \n",
    "# local_machine = {'host':'localhost','port':9200}\n",
    "# es = elasticsearch.Elasticsearch(hosts=[local_machine])\n",
    "# \n",
    "# for i,p in enumerate(posts):  \n",
    "#     es.index(index='nw', doc_type='post', id=i, body=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from count_w import count_words_at_url\n",
    "from bla import bla\n",
    "from redis import Redis\n",
    "from rq import Queue, job\n",
    "q = Queue(connection=Redis(),name='blabla2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = q.enqueue(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50fb7127-4e0d-48cf-ac83-5ddd5f62bf09'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50fb7127-4e0d-48cf-ac83-5ddd5f62bf09'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = job.Job.fetch('50fb7127-4e0d-48cf-ac83-5ddd5f62bf09', connection=Redis())\n",
    "j.is_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nw.parser import NwParser\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from nw.settings import NW_SQL_PATH, ELASTIC_HOSTS\n",
    "import pyelasticsearch\n",
    "es = pyelasticsearch.ElasticSearch()\n",
    "engine = create_engine('sqlite:///{}'.format(NW_SQL_PATH))\n",
    "df = pd.read_sql('select  * from nwdump  limit 1000', engine)\n",
    "parser = NwParser()\n",
    "\n",
    "collect = []\n",
    "for i, topichtml in enumerate(df.topic_html):\n",
    "    if not 'TEMAT SKASOWANY' in topichtml:\n",
    "        topic_json = parser.topic_html_to_json(topic_html=topichtml)[0]\n",
    "        for post in topic_json:\n",
    "            collect.append(post)\n",
    "            \n",
    "            \n",
    "docs = [es.index_op(d, id=d.get('unique_post_id')) for d in collect]\n",
    "es.bulk(docs, index='nw', doc_type='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105575, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame.from_records(collect).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 13.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105575\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'errors': False,\n",
       " 'items': [{'index': {'_id': '173199.15',\n",
       "    '_index': 'nw',\n",
       "    '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
       "    '_type': 'post',\n",
       "    '_version': 9,\n",
       "    'created': False,\n",
       "    'result': 'updated',\n",
       "    'status': 200}}],\n",
       " 'took': 693}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "up = [es.index_op({'a':'b','post_date':datetime.datetime.today()},id='173199.15')]\n",
    "es.bulk(up, index='nw', doc_type='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 106 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cites': [],\n",
       " 'forum_id': '/forum/4',\n",
       " 'post_body': 'Dzisiaj rano wsiadając do auta spotkała mnie niemiła niespodzianka. Przy grzebaniu w bagażniku zorientowałem się że auto ma głęboką rysę idącą przez cały bok (przednie i tylne nadkola, drzwii, łącznie ok. 5 elementów, stratę można liczyć na 2-2,5k)… Nikogo nie zastawiłem ani nie zablokowałem wyjazdów, 10m od skrzyżowania, 1,5m dla pieszych zachowane… Ulica przylega do domku rodzinnego w którym mieszka jakiś Janusz.\\n\\r\\nJanusz ten tydzień wcześniej wyleciał z mordą na moją dziewczynę, kiedy zaparkowała w tym miejscu (innym autem). Z jej opowieści brzmiało to mniej więcej tak:\\n\\r\\n\"NIE PARKUJ TUTAJ, BO JA TU MAM DOM I MUSZĘ TU ODŚNIEŻAĆ I COŚ SIĘ MOŻE TU AUTU STAĆ!\"\\n\\r\\nDodam że nikt nigdy tam nie parkuje (i już wiem czemu) po mimo faktu, że to dobre i legalne miejsce do parkowania.\\n\\r\\nWiadomo że trzeba łamać, pytanie jak? Po pracy idę na policję zgłosić zniszczenie mienia. Z tego co wiem w okolicy nie ma kamer.\\n\\r\\nvideo related:\\n\\nhttps://www.dropbox.com/s/dboc1qmvbg8grhs/Plik%2016.11.2016%...',\n",
       " 'post_date': datetime.datetime(2016, 11, 16, 13, 46, 15),\n",
       " 'post_id': '1',\n",
       " 'topic_id': 173469,\n",
       " 'unique_post_id': '173469.1',\n",
       " 'user_href': '29243',\n",
       " 'user_name': 'Vol'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
