{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_post_bodies(bodies):\n",
    "    for body in bodies:\n",
    "        cites = []\n",
    "        cited = body.findAll('div',{'class':'cite'})\n",
    "        if cited:\n",
    "            cites = []\n",
    "            for c in cited:\n",
    "                cites.append(c['name'])\n",
    "        collect_text = []\n",
    "        for tag in body:\n",
    "            if tag.name not in ('div','p'):\n",
    "                if hasattr(tag,'text'):\n",
    "                    collect_text.append(tag.text)\n",
    "                if isinstance(tag, NavigableString):\n",
    "                    collect_text.append(str(tag))\n",
    "\n",
    "        else:\n",
    "            yield ''.join(collect_text), cites\n",
    "\n",
    "            \n",
    "class NW(object):\n",
    "    BASE_URL = 'http://netwars.pl/'\n",
    "    BASE_URL_TOPIC = 'http://netwars.pl/temat/{!s}'\n",
    "    OT_FORUM_NUMBER = '4'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s(%r)'%(self.__class__, self.username)\n",
    "     \n",
    "    def __init__(self, username=None, password=None):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.logged_in = False\n",
    "        \n",
    "    @staticmethod\n",
    "    def _url_to_soup(url):\n",
    "        return BeautifulSoup(requests.get(url).text,'lxml')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _topic_differences(old, new):\n",
    "        \"\"\"\n",
    "        Return topics that changed between two scrapes\n",
    "        \"\"\"\n",
    "        return dict((set(new.items()) - set(old.items()))).keys()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _live_user_differences(old, new):\n",
    "        return set(old) == set(new)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _topic_soup_to_json(soup):\n",
    "        \n",
    "        if 'Nie znaleziono' in soup.text:\n",
    "            raise ValueError('Topic does not exist')\n",
    "        \n",
    "        topic_number = list(filter(\n",
    "                lambda x: 'topic_' in x, [\n",
    "                    d.get('id','not-relevant') \n",
    "                    for d in soup.findAll('div')\n",
    "                ]))\n",
    "        \n",
    "        \n",
    "        topic_id = int(topic_number[0].split('_')[-1])        \n",
    "        navi_list = [a for a in  soup.findAll('ul', {'class':'forum_navi'})][0].findAll('li')\n",
    "        forum_id = navi_list[1].a['href']\n",
    "        topic_name = navi_list[2].text\n",
    "\n",
    "        \n",
    "        ids = soup.findAll('div',{'class':'post'})\n",
    "        dates = soup.findAll('div',{'class':'p2_data'})\n",
    "        nicks = soup.findAll('div',{'class':'p2_nick'})\n",
    "        bodies  = soup.findAll('div',{'class':'post_body'})\n",
    "\n",
    "        dates = map(lambda x: x.text, dates)\n",
    "        post_bodies = process_post_bodies(bodies)\n",
    "        user_hrefs = map(lambda x: x.a['href'], nicks)\n",
    "        user_names = map(lambda x: x.a.text, nicks)\n",
    "        ids = map(lambda x: x['id'].split('_')[-1], ids)\n",
    "\n",
    "        posts_list = [\n",
    "            {\n",
    "            'topic_id':topic_id,\n",
    "            'forum_id':forum_id,\n",
    "            'post_id':pid, \n",
    "            'post_date':pdate, \n",
    "            'user_href':href, \n",
    "            'user_name':uname, \n",
    "            'post_body':body,\n",
    "            'cites': cites\n",
    "\n",
    "            } for pid,pdate,href,uname, (body, cites) in \n",
    "             zip(ids, dates, user_hrefs, user_names, post_bodies)\n",
    "        ]\n",
    "           \n",
    "        topic_meta = {\n",
    "            'forum_id':forum_id,\n",
    "            'topic_name': topic_name,\n",
    "            'topic_id': topic_id\n",
    "            \n",
    "        }\n",
    "\n",
    "        return posts_list, topic_meta\n",
    "\n",
    "\n",
    "    def login(self):\n",
    "        if not self.username or notself.password:\n",
    "            raise ValueError('No cred')\n",
    "            \n",
    "        payload = {\n",
    "            'tnick':self.username,\n",
    "            'tpass':self.password\n",
    "        }\n",
    "        \n",
    "        nwsession = requests.session()\n",
    "        nwsession.post(urljoin(self.BASE_URL,'login'), payload)\n",
    "        self.nwsession = nwsession\n",
    "        self.logged_in = True\n",
    "    \n",
    "    def logout(self):\n",
    "        self.nwsession.post(urljoin(self.BASE_URL,'logout'))\n",
    "        self.logged_in = False\n",
    "        \n",
    "\n",
    "    def topic_to_json(self, topic_number):\n",
    "        soup = self._url_to_soup(self.BASE_URL_TOPIC.format(topic_number))\n",
    "        return self._topic_soup_to_json(soup)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _list_of_active_users(base_soup):\n",
    "        return [a['href'] for a in base_soup.findAll(\n",
    "                'div',attrs={'id':'footer'}\n",
    "            )[0].findAll('a')][2:]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _topics_and_post_number(base_soup):\n",
    "        topic_ids = [int(z.a['href'].split('/')[-1]) for z in base_soup.findAll('td',{'class':'topic'})]\n",
    "        number_of_posts = [int(z.text) for z in s.findAll('td',{'class':'posts'})]\n",
    "        return dict(zip(topic_ids, number_of_posts))\n",
    "    \n",
    "    def home_page_status(self):\n",
    "        soup = self._url_to_soup(self.BASE_URL)\n",
    "        topics = self._topics_and_post_number(soup)\n",
    "        users = self._list_of_active_users(soup)\n",
    "        return topics, users\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw = NW()\n",
    "post_list, meta = nw.topic_to_json(173469)\n",
    "k = nw.home_page_status()\n",
    "a, b = nw.home_page_status()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, not 'bytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-391e433251be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mr\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mredis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRedis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/i008/anaconda3/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         raise TypeError('the JSON object must be str, not {!r}'.format(\n\u001b[1;32m--> 312\u001b[1;33m                             s.__class__.__name__))\n\u001b[0m\u001b[0;32m    313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'\\ufeff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         raise JSONDecodeError(\"Unexpected UTF-8 BOM (decode using utf-8-sig)\",\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, not 'bytes'"
     ]
    }
   ],
   "source": [
    "import redis \n",
    "import json\n",
    "r  = redis.Redis()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1,'b':2,'c':3}\n",
    "b = {'a':2,'b':2,'c':3,'d':1, 'x':12}\n",
    "\n",
    "\n",
    "set_a = set(a.items())\n",
    "set_b = set(b.items())\n",
    "\n",
    "dict((set_b - set_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i008/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/user/27684',\n",
       " '/user/16893',\n",
       " '/user/28070',\n",
       " '/user/29418',\n",
       " '/user/31530',\n",
       " '/user/31908',\n",
       " '/user/7743',\n",
       " '/user/29465',\n",
       " '/user/16019',\n",
       " '/user/29851',\n",
       " '/user/29787',\n",
       " '/user/6046',\n",
       " '/user/31207',\n",
       " '/user/32012',\n",
       " '/user/26840',\n",
       " '/user/9877',\n",
       " '/user/27917',\n",
       " '/user/31143',\n",
       " '/user/5923',\n",
       " '/user/10734',\n",
       " '/user/21998',\n",
       " '/user/32039',\n",
       " '/user/29358',\n",
       " '/user/18347',\n",
       " '/user/15662',\n",
       " '/user/32137',\n",
       " '/user/17841',\n",
       " '/user/808',\n",
       " '/user/4093',\n",
       " '/user/31924',\n",
       " '/user/15013',\n",
       " '/user/27459',\n",
       " '/user/13142',\n",
       " '/user/11224',\n",
       " '/user/9926',\n",
       " '/user/7201',\n",
       " '/user/398',\n",
       " '/user/4053',\n",
       " '/user/28771',\n",
       " '/user/12677',\n",
       " '/user/31646',\n",
       " '/user/27782',\n",
       " '/user/28071',\n",
       " '/user/2656',\n",
       " '/user/31392',\n",
       " '/user/27403',\n",
       " '/user/584',\n",
       " '/user/9649']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytz import utc\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "from apscheduler.jobstores.redis import RedisJobStore\n",
    "from apscheduler.executors.pool import ProcessPoolExecutor\n",
    "\n",
    "jobstores = {\n",
    "#     'mongo': {'type': 'mongodb'},\n",
    "#     'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "    'default': RedisJobStore()\n",
    "}\n",
    "executors = {\n",
    "    'default': {'type': 'threadpool', 'max_workers': 20},\n",
    "    'processpool': ProcessPoolExecutor(max_workers=5)\n",
    "}\n",
    "\n",
    "job_defaults = {\n",
    "    'coalesce': False,\n",
    "    'max_instances': 3\n",
    "}\n",
    "# scheduler = BlockingScheduler()\n",
    "scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "import hashlib\n",
    "import requests\n",
    "L = []\n",
    "def get_nw_hash():\n",
    "    hash_string = hashlib.md5(requests.get('http://netwars.pl').text.encode('utf8')).hexdigest()\n",
    "    \n",
    "    if hash_string in L:\n",
    "        L.append('oh now nw changed!')\n",
    "    else:\n",
    "        L.append(hash_string)    \n",
    "\n",
    "scheduler.add_job(get_nw_hash, 'interval', seconds=30)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytz import utc\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore\n",
    "from apscheduler.jobstores.redis import RedisJobStore\n",
    "from apscheduler.executors.pool import ProcessPoolExecutor\n",
    "\n",
    "jobstores = {\n",
    "#     'mongo': {'type': 'mongodb'},\n",
    "#     'default': SQLAlchemyJobStore(url='sqlite:///jobs.sqlite')\n",
    "    'default': RedisJobStore()\n",
    "}\n",
    "executors = {\n",
    "    'default': {'type': 'threadpool', 'max_workers': 20},\n",
    "    'processpool': ProcessPoolExecutor(max_workers=5)\n",
    "}\n",
    "\n",
    "job_defaults = {\n",
    "    'coalesce': False,\n",
    "    'max_instances': 3\n",
    "}\n",
    "# scheduler = BlockingScheduler()\n",
    "scheduler = BackgroundScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults, timezone=utc)\n",
    "import hashlib\n",
    "import requests\n",
    "L = []\n",
    "def get_nw_hash():\n",
    "    hash_string = hashlib.md5(requests.get('http://netwars.pl').text.encode('utf8')).hexdigest()\n",
    "    \n",
    "    if hash_string in L:\n",
    "        L.append('oh now nw changed!')\n",
    "    else:\n",
    "        L.append(hash_string)    \n",
    "\n",
    "scheduler.add_job(get_nw_hash, 'interval', seconds=30)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function process_post_body at 0x7ff3a6892d08>\n"
     ]
    }
   ],
   "source": [
    "print(process_post_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-9d53135f39f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0municode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search, Q, MultiSearch\n",
    "import json\n",
    "import subprocess\n",
    "import logstash\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nw = NW()\n",
    "posts, head = nw.topic_to_json(173399)\n",
    "\n",
    "\n",
    "# topic_number = list(filter(\n",
    "#                 lambda x: 'topic_' in x, [\n",
    "#                     d.get('id','not-relevant') \n",
    "#                     for d in soup.findAll('div')\n",
    "#                 ]))\n",
    "# topic_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "local_machine = {'host':'localhost','port':9200}\n",
    "es = elasticsearch.Elasticsearch(hosts=[local_machine])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,p in enumerate(posts):  \n",
    "    es.index(index='nw', doc_type='post', id=i, body=p)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
